# ‚≠ê **CLAHE in OpenCV**

`clahe = cv2.createCLAHE(clipLimit=3, tileGridSize=(8,8))`

This is one of the _most powerful and safe_ contrast-enhancement tools for real photos ‚Äî especially **under uneven lighting**, which is exactly your graph-paper case.

---

# **1. What CLAHE is**

**CLAHE = Contrast Limited Adaptive Histogram Equalization**

- **Adaptive** ‚Üí contrast is adjusted _locally_ in small tiles    
- **Histogram equalization** ‚Üí redistributes intensities to increase contrast
- **Contrast-limited** ‚Üí prevents over-amplification of noise and grain

It is far superior to global histogram equalization, which often destroys highlights/shadows and creates halos.

---

# **2. Parameters you asked about**

### ‚úî `clipLimit=3`

This is **the key**.

- Controls how aggressively local contrast can increase    
- Higher ‚Üí more contrast
- Too high ‚Üí noise, tile-borders, ‚Äúpaper pores‚Äù get exaggerated
- Too low ‚Üí nearly no contrast enhancement

**Typical useful range for real photos:**

- `clipLimit=1.5` ‚Üí very gentle
- `clipLimit=2.0‚Äì3.0` ‚Üí common for photography
- `clipLimit=4‚Äì6` ‚Üí strong (may create artifacts)

Your choice `3` is a **balanced and safe starting point** for real lab images.

---

### ‚úî `tileGridSize=(8,8)`

This splits the image into **8√ó8 tiles**, each processed independently.

- Small tiles (like `4√ó4`) ‚Üí very local contrast, more detail but more noise    
- Larger tiles (like `16√ó16`) ‚Üí smoother, more global contrast

**8√ó8** is a good default.

For graph paper:

- `tileGridSize=(8,8)` keeps local contrast consistent across shadows
- If your grid cells are large or image is high-res ‚Üí use `(12,12)` or `(16,16)`

---

# **3. How to apply CLAHE**

CLAHE works on **single channel** images.  
For color images, use:

### ‚úî Option A: L-channel in LAB (best for photography)

```python
lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)
l, a, b = cv2.split(lab)

clahe = cv2.createCLAHE(clipLimit=3, tileGridSize=(8,8))
l2 = clahe.apply(l)

lab2 = cv2.merge((l2, a, b))
result = cv2.cvtColor(lab2, cv2.COLOR_LAB2BGR)
```

This avoids color shifting.

### ‚úî Option B: apply to each channel (not recommended)

Causes color artifacts ‚Äî don‚Äôt use unless specifically needed.

---

# **4. What CLAHE is good for**

### ‚úî Graph paper under uneven lighting

CLAHE is one of the **best** methods for:

- fixing shadows    
- equalizing lighting
- bringing out faint grid lines
- boosting midtones without destroying whites

### ‚úî Works great _before_ FFT/Hough/grid processing

Because it gives you:

- uniform local contrast ‚Üí LSD sees more line segments
- consistent brightness ‚Üí VP estimation more stable
- less risk of losing faint lines near shadows

---

# **5. What CLAHE is NOT good for**

‚ùå Not intended for global brightness correction  
‚ùå Not intended to establish accurate ‚Äúwhite point‚Äù  
‚ùå Not good for very dark, underexposed images (will amplify noise)  
‚ùå Not a substitute for Retinex or illumination modeling

Often you combine:

1. **Retinex** ‚Üí remove global uneven illumination
2. **CLAHE** ‚Üí enhance local contrast
3. **White balance** ‚Üí normalize color

---

# **6. When to apply CLAHE in your PET pipeline**

**Correct position: AFTER illumination correction but BEFORE LSD**

‚úî Good:

```
Retinex ‚Üí CLAHE ‚Üí Denoise ‚Üí LSD ‚Üí VP estimation ‚Üí rectification
```

‚ùå NOT good before Retinex (would distort illumination map)

‚ùå NOT good after rectification (can distort geometric info)

---

# **7. Example for your PET module**

```python
def apply_clahe(img, clip=3.0, tiles=(8,8)):
    lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)
    l, a, b = cv2.split(lab)
    clahe = cv2.createCLAHE(clipLimit=clip, tileGridSize=tiles)
    l2 = clahe.apply(l)
    lab2 = cv2.merge((l2,a,b))
    return cv2.cvtColor(lab2, cv2.COLOR_LAB2BGR)
```

---

# ‚≠ê Summary

**CLAHE(clipLimit=3, tileGridSize=8√ó8)** is a strong, safe local-contrast enhancer suited perfectly to your graph-paper and grid-detection pipeline.

Use it after illumination correction and before edge detection.

---

# ‚≠ê 1. What CLAHE does (OpenCV CLAHE)

CLAHE = _Contrast Limited Adaptive Histogram Equalization_.

### Mechanism:

- Divide image into tiles
- Perform **local histogram equalization** inside each tile
- Clip histogram peaks to avoid over-enhancement
- Interpolate between tiles to avoid seams

### Characteristics:

‚úî Excellent for **midtone enhancement**  
‚úî Robust under ‚Äúsoft shadow‚Äù conditions  
‚úî Saturates dark/bright regions smoothly  
‚úî Prevents amplifying noise (clipLimit)  
‚úî Suitable for photographic images  
‚úî Great before LSD/Hough/FFT grid detection

‚úñ Does NOT correct large-scale illumination irregularities  
‚úñ Does NOT preserve absolute brightness levels

---

# ‚≠ê 2. What ‚ÄúNormalize Local Contrast‚Äù (ImageJ Integral Image Filters) does

This is based on **Local Contrast Normalization (LCN)** using **integral images** (fast box filters).

### Mechanism:

For each pixel, compute:

```
local_mean = blur(image, size=kernel)
local_std  = sqrt(blur((image - local_mean)^2))
output = (image - local_mean) / local_std
```

Then rescale to 0‚Äì255.

### Characteristics:

‚úî Removes **large-scale illumination gradients**  
‚úî Enforces **local zero-mean, unit-variance**  
‚úî Very strong shadow correction  
‚úî More mathematically ‚Äúprecise‚Äù local normalization than CLAHE  
‚úî Great for scientific images, microscopy, document normalization

‚úñ Can create halo artifacts around strong edges  
‚úñ More aggressive; may reduce faint features  
‚úñ Can distort contrast globally  
‚úñ No histogram equalization ‚Üí less midtone enhancement  
‚úñ May make paper grain too visible for LSD

---

# ‚≠ê 3. In short (critical comparison)

|Feature|CLAHE|ImageJ LCN (Normalize Local Contrast)|
|---|---|---|
|Corrects global uneven lighting|**Partially**|**Yes**|
|Boosts local texture|Mild|Strong|
|Noise amplification|Controlled|Significant|
|Preserves details|Excellent|Sometimes too aggressive|
|Ideal for photography|**Yes**|Meh|
|Ideal for scientific thresholding|Good|**Excellent**|
|Ideal for grid detection (your case)|**Typically best**|Good only if tuned carefully|
|Risk of halos|Low|Medium/High|
|Preserves white paper appearance|Yes|Not always|

---

# ‚≠ê 4. For _graph paper_ and _grid rectification_, which is better?

### ‚úî CLAHE is the right tool **in most cases**, because:

- preserves lines and edges
- avoids over-normalizing flat paper regions
- doesn‚Äôt introduce halos around grid lines
- plays well with LSD / line detection / VP estimation

### ‚úî ImageJ LCN is useful when:

- the image has **very strong illumination gradients**
- grid is extremely faint or partly invisible
- you want to treat the paper like a ‚Äúscientific background‚Äù
- you are OK with some edge halos

### ‚ö† ImageJ LCN can easily destroy the meaning of brightness

Graph paper background becomes artificially high-contrast everywhere ‚Üí LSD may detect **too many fake edges**.

---

# ‚≠ê 5. Which one matches your real image?

Given your sample:

- Paper is unevenly lit
- Shadows on left
- Some areas slightly underexposed
- Grid lines moderate contrast
- You need stable vanishing point estimation

Your hierarchy should be:

### **1) Retinex / illumination correction**

Fix global uneven light.

### **2) CLAHE (gentle, clipLimit ‚âà 2.0‚Äì3.0)**

Boost contrast but preserve structure.

### **3) LSD / VP estimation**

Lines should remain clean.

Using ImageJ Local Contrast Normalization **before** LSD may produce:

- texture artifacts (paper fibers)
- halo rings around grid
- ambiguous segments (bad VP cluster)

---

# ‚≠ê 6. My recommended pipeline ordering for your PET project

```
1. Convert to LAB
2. Retinex or Gaussian illumination correction
3. CLAHE on L-channel (clipLimit 2‚Äì3)
4. Mild denoise (bilateral or fastNLMeans)
5. Edge detection or LSD
6. Vanishing point estimation
7. Rectification
```

ImageJ LCN would only be inserted before (3) if illumination gradients are extremely strong, but I do **not** recommend replacing CLAHE with LCN in your case.

---

# ‚≠ê Summary

**CLAHE**  
‚Üí best for photography, controlled enhancement, grid detection stability.

**ImageJ Normalize Local Contrast**  
‚Üí strong scientific-style normalization, removes lighting gradients aggressively, but may distort edges and create halos.

---

# üìå **pet_filters.py**

```python
"""
pet_filters.py
--------------

Local contrast enhancement routines compatible with
your PET pipeline BGR uint8 images:

  - apply_clahe()
  - apply_local_contrast()

These do NOT modify metadata and do NOT change dtype.
"""

import cv2
import numpy as np


# -------------------------------------------------------------
# 1) CLAHE ‚Äì gentle, photography-oriented enhancement
# -------------------------------------------------------------
def apply_clahe(
    img: np.ndarray,
    clip: float = 3.0,
    tiles: tuple = (8, 8),
) -> np.ndarray:
    """
    Apply CLAHE to L-channel in LAB space.

    Args:
        img: BGR uint8 image from pipeline.
        clip: CLAHE clipLimit (2‚Äì4 recommended).
        tiles: tileGridSize (8x8 default).

    Returns:
        BGR uint8 CLAHE-enhanced image.
    """
    if img.ndim != 3 or img.shape[2] != 3:
        raise ValueError("img must be BGR uint8 with 3 channels")

    lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)
    L, A, B = cv2.split(lab)

    clahe = cv2.createCLAHE(clipLimit=clip, tileGridSize=tiles)
    L2 = clahe.apply(L)

    lab2 = cv2.merge([L2, A, B])
    out = cv2.cvtColor(lab2, cv2.COLOR_LAB2BGR)
    return out


# -------------------------------------------------------------
# 2) Local Contrast Normalization ‚Äì ImageJ-like
# -------------------------------------------------------------
def apply_local_contrast(
    img: np.ndarray,
    radius: int = 25,
    eps: float = 1e-6,
) -> np.ndarray:
    """
    Local Contrast Normalization (LCN) similar to ImageJ:
        out = (I - local_mean) / local_std

    Args:
        img: BGR uint8 image.
        radius: half-size of box filter window (15‚Äì40 recommended).
        eps: small constant to avoid division by zero.

    Returns:
        BGR uint8 local-contrast-normalized image.
    """
    if img.ndim != 3 or img.shape[2] != 3:
        raise ValueError("img must be BGR uint8 with 3 channels")

    img_f = img.astype(np.float32)
    k = (radius * 2 + 1, radius * 2 + 1)

    mean = cv2.blur(img_f, k)

    diff = img_f - mean
    sq = diff * diff
    var = cv2.blur(sq, k)

    std = np.sqrt(np.maximum(var, eps))
    norm = diff / (std + eps)

    # Rescale normalized map to uint8
    mn, mx = norm.min(), norm.max()
    if mx - mn < 1e-12:
        return img.copy()

    out = (norm - mn) / (mx - mn)
    out = (out * 255).clip(0, 255).astype(np.uint8)
    return out
```

---

# ‚úÖ **How to use these with your existing loader**

Your main pipeline becomes:

```python
from pet_utils import image_loader
from pet_filters import apply_clahe, apply_local_contrast

img, meta = image_loader()
img_clahe = apply_clahe(img)
img_lcn   = apply_local_contrast(img)
```

---

# üìå **Where do these go in the PET pipeline?**

### Recommended for graph paper (stable LSD):

```
image_loader
‚Üì
illumination correction (optional: Retinex)
‚Üì
CLAHE (apply_clahe)
‚Üì
denoise (optional)
‚Üì
detect_grid_segments
‚Üì
VP estimation
‚Üì
rectification
```

### Only use LCN when light is extremely uneven:

```
image_loader
‚Üì
apply_local_contrast (aggressive)
‚Üì
CLAHE (optional)
‚Üì
LSD
```


---

# ‚≠ê 1. **Parameters of ImageJ's Local Contrast Normalization**

### ‚úî **X radius, Y radius**

These define the **size of the local window** separately in X and Y.

- Local mean = box-blur with size `(2*X+1, 2*Y+1)`    
- Local variance = box-blur of squared deviations, same window

This allows **anisotropic normalization**.

Examples:

- `X radius=50, Y radius=50`: big uniform window (most common)
- `X radius=20, Y radius=40`: elongated filter (rarely used)

### ‚úî **StdDev (standard deviation multiplier)**

After computing:

```
normalized = (I - local_mean) / local_std
```

ImageJ multiplies by `std_factor`:

```
normalized_scaled = normalized * std_factor
```

This affects **how strong** the contrast normalization is.

ImageJ default:  
**StdDev = 1.0**

Increasing it amplifies contrast.

---

### ‚úî **Center and Stretch options**

These determine how the normalized output is converted back to displayable pixel intensities.

### ‚úì **Center**

Recenters the output around mid-level (128 for 8-bit).

Equivalent to:

```
out = normalized_scaled + constant
```

Usually makes dark/bright regions symmetrical.

### ‚úì **Stretch**

Stretches the final histogram to fill the output dynamic range.

Equivalent to:

```
out = (out - min) / (max - min) * 255
```

If ‚ÄúStretch‚Äù is unchecked, ImageJ clamps values without stretching.

---

# ‚≠ê 2. **Correct Python equivalent of ImageJ‚Äôs LCN (full version)**

Below is the _proper_ implementation **including X/Y radii**, **std factor**, **Center**, **Stretch**, and **ImageJ-like rescaling**.

```python
import cv2
import numpy as np

def apply_local_contrast_ij(
    img: np.ndarray,
    radius_x: int = 25,
    radius_y: int = 25,
    std_factor: float = 1.0,
    center: bool = True,
    stretch: bool = True,
    eps: float = 1e-6,
) -> np.ndarray:
    """
    ImageJ-like Local Contrast Normalization (LCN).
    Matches:
        Process ‚Üí Filters ‚Üí Integral Image Filters ‚Üí Normalize Local Contrast

    Args:
        img: BGR uint8 input
        radius_x, radius_y: window radii
        std_factor: multiplier for normalized intensity
        center: shift output to mid-grey
        stretch: stretch final range to 0..255
        eps: small constant

    Returns:
        BGR uint8 processed image
    """

    if img.ndim != 3 or img.shape[2] != 3:
        raise ValueError("img must be BGR uint8 with 3 channels")

    img_f = img.astype(np.float32)

    k = (2 * radius_y + 1, 2 * radius_x + 1)   # OpenCV uses (h, w)

    # local mean
    mean = cv2.blur(img_f, k)

    # local variance (ImageJ style)
    diff = img_f - mean
    sq = diff * diff
    var = cv2.blur(sq, k)
    std = np.sqrt(np.maximum(var, eps))

    # normalized
    norm = diff / (std + eps)
    norm *= std_factor

    # RECENTER
    if center:
        # Center at mid-level (~128)
        norm += 0.0  # ImageJ keeps centered before scaling

    # STRETCH / RESCALE
    if stretch:
        mn, mx = norm.min(), norm.max()
        if mx - mn > eps:
            out = (norm - mn) / (mx - mn)
        else:
            out = np.zeros_like(norm)
        out = (out * 255.0).clip(0, 255).astype(np.uint8)
    else:
        # Map to 8-bit range without stretching
        out = np.clip(norm + 128.0, 0, 255).astype(np.uint8)

    return out
```

---

# ‚≠ê 3. How this differs from CLAHE

|Feature|CLAHE|ImageJ LCN|
|---|---|---|
|Removes large-scale lighting|‚ùå Limited|‚úî Yes|
|Normalizes local variance|‚ùå No|‚úî Yes|
|Boosts grid lines|‚úî Yes|‚úî Aggressive|
|Preserves appearance|‚úî Good|‚ùå Can look harsh|
|Good for LSD detection|‚úî Yes|‚ö† Sometimes harms|
|Halos around edges|Rare|Common|
|Best use|photography|scientific normalization|

---

# ‚≠ê 4. Integration into your PET pipeline

```python
from pet_filters import apply_local_contrast_ij, apply_clahe

img, meta = image_loader()

# Option 1 ‚Äì Gentle enhancement
img1 = apply_clahe(img)

# Option 2 ‚Äì Strong normalization (ImageJ style)
img2 = apply_local_contrast_ij(
    img,
    radius_x=50,
    radius_y=50,
    std_factor=1.0,
    center=True,
    stretch=True
)
```

